{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 550152\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('snli')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_train_dataset = prepare_eval_dataset = \\\n",
    "            lambda exs: prepare_dataset_nli(exs, tokenizer, args.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "trans = str.maketrans('', '', string.punctuation)\n",
    "def remove_stop_count_words(df):\n",
    "    df['premise_wo_stopwords'] = df['premise'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]).strip().lower().translate(trans))\n",
    "    return df.premise_wo_stopwords.str.split(expand=True).stack().value_counts()\n",
    "\n",
    "train_df = train_dataset.to_pandas()\n",
    "\n",
    "entail_df = train_df[train_df.label == 0]\n",
    "neutral_df = train_df[train_df.label == 1]\n",
    "contradict_df = train_df[train_df.label == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1193/2886799954.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['premise_wo_stopwords'] = df['premise'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]).strip().lower().translate(trans))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>103933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>50342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>26126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>two</td>\n",
       "      <td>25263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wearing</td>\n",
       "      <td>20078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19671</th>\n",
       "      <td>pocketbooks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19672</th>\n",
       "      <td>elevates</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19673</th>\n",
       "      <td>headtotoe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19674</th>\n",
       "      <td>illusion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19675</th>\n",
       "      <td>nailgloss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19676 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index       0\n",
       "0                a  103933\n",
       "1              man   50342\n",
       "2            woman   26126\n",
       "3              two   25263\n",
       "4          wearing   20078\n",
       "...            ...     ...\n",
       "19671  pocketbooks       1\n",
       "19672     elevates       1\n",
       "19673    headtotoe       1\n",
       "19674     illusion       1\n",
       "19675    nailgloss       1\n",
       "\n",
       "[19676 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entail_counts = remove_stop_count_words(entail_df).reset_index()\n",
    "entail_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1193/2886799954.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['premise_wo_stopwords'] = df['premise'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]).strip().lower().translate(trans))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>103611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>50143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>26108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>two</td>\n",
       "      <td>25114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>20068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19624</th>\n",
       "      <td>nemo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19625</th>\n",
       "      <td>laundered</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19626</th>\n",
       "      <td>joys</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19627</th>\n",
       "      <td>lecturers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19628</th>\n",
       "      <td>nailgloss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19629 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index       0\n",
       "0              a  103611\n",
       "1            man   50143\n",
       "2          woman   26108\n",
       "3            two   25114\n",
       "4         people   20068\n",
       "...          ...     ...\n",
       "19624       nemo       1\n",
       "19625  laundered       1\n",
       "19626       joys       1\n",
       "19627  lecturers       1\n",
       "19628  nailgloss       1\n",
       "\n",
       "[19629 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_counts = remove_stop_count_words(neutral_df).reset_index()\n",
    "neutral_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1193/2886799954.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['premise_wo_stopwords'] = df['premise'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]).strip().lower().translate(trans))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>103847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>man</td>\n",
       "      <td>50285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>woman</td>\n",
       "      <td>26117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>two</td>\n",
       "      <td>25209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wearing</td>\n",
       "      <td>20054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19679</th>\n",
       "      <td>craftslady</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680</th>\n",
       "      <td>gasping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>gasp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>portopotties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19683</th>\n",
       "      <td>nailgloss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19684 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              index       0\n",
       "0                 a  103847\n",
       "1               man   50285\n",
       "2             woman   26117\n",
       "3               two   25209\n",
       "4           wearing   20054\n",
       "...             ...     ...\n",
       "19679    craftslady       1\n",
       "19680       gasping       1\n",
       "19681          gasp       1\n",
       "19682  portopotties       1\n",
       "19683     nailgloss       1\n",
       "\n",
       "[19684 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contradict_counts = remove_stop_count_words(contradict_df).reset_index()\n",
    "contradict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, 0]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entail_counts[~entail_counts.index.isin(contradict_counts.index)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19629</th>\n",
       "      <td>alsosleeping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19630</th>\n",
       "      <td>expats</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19631</th>\n",
       "      <td>composure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19632</th>\n",
       "      <td>regains</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19633</th>\n",
       "      <td>profusely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19634</th>\n",
       "      <td>4315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19635</th>\n",
       "      <td>complexioned</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19636</th>\n",
       "      <td>guidebooks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19637</th>\n",
       "      <td>cataloging</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19638</th>\n",
       "      <td>dayhiker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19639</th>\n",
       "      <td>woodenpaneled</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19640</th>\n",
       "      <td>reenacting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19641</th>\n",
       "      <td>awash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19642</th>\n",
       "      <td>carcasses</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19643</th>\n",
       "      <td>churchs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>larch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>ratty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>glasgow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>primes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>user</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19649</th>\n",
       "      <td>jays</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19650</th>\n",
       "      <td>immediately</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19651</th>\n",
       "      <td>grimly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19652</th>\n",
       "      <td>atari</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19653</th>\n",
       "      <td>ourense</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19654</th>\n",
       "      <td>sorvette</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19655</th>\n",
       "      <td>nicolau</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19656</th>\n",
       "      <td>sorvete</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>kenji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>fringes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>muralcovered</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>yellowfringed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19661</th>\n",
       "      <td>enable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19662</th>\n",
       "      <td>sentence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19663</th>\n",
       "      <td>elastic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19664</th>\n",
       "      <td>installations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19665</th>\n",
       "      <td>enact</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19666</th>\n",
       "      <td>objectives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19667</th>\n",
       "      <td>pooping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19668</th>\n",
       "      <td>picutre</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19669</th>\n",
       "      <td>purpose</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19670</th>\n",
       "      <td>formers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19671</th>\n",
       "      <td>pocketbooks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19672</th>\n",
       "      <td>elevates</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19673</th>\n",
       "      <td>headtotoe</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19674</th>\n",
       "      <td>illusion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19675</th>\n",
       "      <td>nailgloss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  0\n",
       "19629   alsosleeping  1\n",
       "19630         expats  1\n",
       "19631      composure  1\n",
       "19632        regains  1\n",
       "19633      profusely  1\n",
       "19634           4315  1\n",
       "19635   complexioned  1\n",
       "19636     guidebooks  1\n",
       "19637     cataloging  1\n",
       "19638       dayhiker  1\n",
       "19639  woodenpaneled  1\n",
       "19640     reenacting  1\n",
       "19641          awash  1\n",
       "19642      carcasses  1\n",
       "19643        churchs  1\n",
       "19644          larch  1\n",
       "19645          ratty  1\n",
       "19646        glasgow  1\n",
       "19647         primes  1\n",
       "19648           user  1\n",
       "19649           jays  1\n",
       "19650    immediately  1\n",
       "19651         grimly  1\n",
       "19652          atari  1\n",
       "19653        ourense  1\n",
       "19654       sorvette  1\n",
       "19655        nicolau  1\n",
       "19656        sorvete  1\n",
       "19657          kenji  1\n",
       "19658        fringes  1\n",
       "19659   muralcovered  1\n",
       "19660  yellowfringed  1\n",
       "19661         enable  1\n",
       "19662       sentence  1\n",
       "19663        elastic  1\n",
       "19664  installations  1\n",
       "19665          enact  1\n",
       "19666     objectives  1\n",
       "19667        pooping  1\n",
       "19668        picutre  1\n",
       "19669        purpose  1\n",
       "19670        formers  1\n",
       "19671    pocketbooks  1\n",
       "19672       elevates  1\n",
       "19673      headtotoe  1\n",
       "19674       illusion  1\n",
       "19675      nailgloss  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entail_counts[~entail_counts.index.isin(neutral_counts.index)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19676</th>\n",
       "      <td>clutter</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19677</th>\n",
       "      <td>lawns</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19678</th>\n",
       "      <td>kentucky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19679</th>\n",
       "      <td>craftslady</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680</th>\n",
       "      <td>gasping</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>gasp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>portopotties</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19683</th>\n",
       "      <td>nailgloss</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index  0\n",
       "19676       clutter  1\n",
       "19677         lawns  1\n",
       "19678      kentucky  1\n",
       "19679    craftslady  1\n",
       "19680       gasping  1\n",
       "19681          gasp  1\n",
       "19682  portopotties  1\n",
       "19683     nailgloss  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contradict_counts[~contradict_counts.index.isin(entail_counts.index)].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/richiew/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expanded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usedly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>equalable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rotter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phototelescopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234372</th>\n",
       "      <td>strickenly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234373</th>\n",
       "      <td>falconbill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234374</th>\n",
       "      <td>zoophaga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234375</th>\n",
       "      <td>squirk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234376</th>\n",
       "      <td>benzophenanthrazine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234377 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      words\n",
       "0                  expanded\n",
       "1                    usedly\n",
       "2                 equalable\n",
       "3                    rotter\n",
       "4           phototelescopic\n",
       "...                     ...\n",
       "234372           strickenly\n",
       "234373           falconbill\n",
       "234374             zoophaga\n",
       "234375               squirk\n",
       "234376  benzophenanthrazine\n",
       "\n",
       "[234377 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "words_set = list(set([x.lower() for x in words.words()]))\n",
    "words_df = pd.DataFrame()\n",
    "words_df['words'] = words_set\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expanded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usedly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>equalable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rotter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phototelescopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234372</th>\n",
       "      <td>strickenly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234373</th>\n",
       "      <td>falconbill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234374</th>\n",
       "      <td>zoophaga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234375</th>\n",
       "      <td>squirk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234376</th>\n",
       "      <td>benzophenanthrazine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224142 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      words\n",
       "0                  expanded\n",
       "1                    usedly\n",
       "2                 equalable\n",
       "3                    rotter\n",
       "4           phototelescopic\n",
       "...                     ...\n",
       "234372           strickenly\n",
       "234373           falconbill\n",
       "234374             zoophaga\n",
       "234375               squirk\n",
       "234376  benzophenanthrazine\n",
       "\n",
       "[224142 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df[~words_df.words.isin(contradict_counts['index'])].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expanded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usedly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>equalable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rotter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phototelescopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234372</th>\n",
       "      <td>strickenly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234373</th>\n",
       "      <td>falconbill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234374</th>\n",
       "      <td>zoophaga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234375</th>\n",
       "      <td>squirk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234376</th>\n",
       "      <td>benzophenanthrazine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>224151 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      words\n",
       "0                  expanded\n",
       "1                    usedly\n",
       "2                 equalable\n",
       "3                    rotter\n",
       "4           phototelescopic\n",
       "...                     ...\n",
       "234372           strickenly\n",
       "234373           falconbill\n",
       "234374             zoophaga\n",
       "234375               squirk\n",
       "234376  benzophenanthrazine\n",
       "\n",
       "[224151 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df[~words_df.words.isin(entail_counts['index'])].dropna().to_csv('entail-missing-words.csv')\n",
    "words_df[~words_df.words.isin(entail_counts['index'])].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWords that I might be able to use:\\nliquer\\ndecompress\\nfeminize\\nairway\\nuniformness\\nguesswork\\ndecently\\ncourteous\\nenergetics\\nbiomathematics\\ncommentary\\ninterpreter\\nparallelizer\\nunequitably\\ndoghood\\ndutiful\\nslabbed\\nliberative\\nnondivisible\\nalt\\nhungerless\\nencrypt\\nunproportional\\nbrainsickness\\nforgetting\\nsqueaky\\nfad\\nradiate\\nhoarder\\nearful\\n\\nUsed:\\ndowery\\ncriteria\\nsurfacing\\nupbeat\\nunagressive\\nemotionless\\nbiomathematics\\nairway \\nsqueaky\\nencrypt\\ninterpreter\\ncommentary\\n\\nentailment examples:\\n\\nA bride's family gifts the husband money in exchange for marriage\\nThe union had the criteria of a dowery\\n\\nThe vessel rises above the ocean\\nA submarine sufacing makes a big splash\\n\\nA group of friends dance to music\\nAn upbeat song is playing for people\\n\\nA security guard calmly escorts the person off the property\\nThe patrol is unaggressive\\n\\nThe girl stares off into the sunset in thought\\nSomeone appears emotionless as the day ends\\n\\nThe graduate holds a biomathematics degree on stage\\nHe is ready to work as a medical researcher\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Words that I might be able to use:\n",
    "liquer\n",
    "decompress\n",
    "feminize\n",
    "airway\n",
    "uniformness\n",
    "guesswork\n",
    "decently\n",
    "courteous\n",
    "energetics\n",
    "biomathematics\n",
    "commentary\n",
    "interpreter\n",
    "parallelizer\n",
    "unequitably\n",
    "doghood\n",
    "dutiful\n",
    "slabbed\n",
    "liberative\n",
    "nondivisible\n",
    "alt\n",
    "hungerless\n",
    "encrypt\n",
    "unproportional\n",
    "brainsickness\n",
    "forgetting\n",
    "squeaky\n",
    "fad\n",
    "radiate\n",
    "hoarder\n",
    "earful\n",
    "\n",
    "Used:\n",
    "dowery\n",
    "criteria\n",
    "surfacing\n",
    "upbeat\n",
    "unagressive\n",
    "emotionless\n",
    "biomathematics\n",
    "airway \n",
    "squeaky\n",
    "encrypt\n",
    "interpreter\n",
    "commentary\n",
    "\n",
    "entailment examples:\n",
    "\n",
    "A bride's family gifts the husband money in exchange for marriage\n",
    "The union had the criteria of a dowery\n",
    "\n",
    "The vessel rises above the ocean\n",
    "A submarine sufacing makes a big splash\n",
    "\n",
    "A group of friends dance to music\n",
    "An upbeat song is playing for people\n",
    "\n",
    "A security guard calmly escorts the person off the property\n",
    "The patrol is unaggressive\n",
    "\n",
    "The girl stares off into the sunset in thought\n",
    "Someone appears emotionless as the day ends\n",
    "\n",
    "The graduate holds a biomathematics degree on stage\n",
    "He is ready to work as a medical researcher\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    "    AutoModelForQuestionAnswering, Trainer, TrainingArguments, HfArgumentParser\n",
    "from helpers import prepare_dataset_nli, prepare_train_dataset_qa, \\\n",
    "    prepare_validation_dataset_qa, QuestionAnsweringTrainer, compute_accuracy\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def eval_model(dataset):\n",
    "    with open('eval_args.pkl', 'rb') as filehandler:\n",
    "        training_args = pickle.load(filehandler)\n",
    "    with open('args.pkl', 'rb') as filehandler:\n",
    "        args = pickle.load(filehandler)\n",
    "\n",
    "    default_datasets = {'qa': ('squad',), 'nli': ('snli',)}\n",
    "    dataset_id = tuple(args.dataset.split(':')) if args.dataset is not None else \\\n",
    "        default_datasets[args.task]\n",
    "    # MNLI has two validation splits (one with matched domains and one with mismatched domains). Most datasets just have one \"validation\" split\n",
    "    eval_split = 'validation_matched' if dataset_id == ('glue', 'mnli') else 'validation'\n",
    "\n",
    "    # NLI models need to have the output label count specified (label 0 is \"entailed\", 1 is \"neutral\", and 2 is \"contradiction\")\n",
    "    task_kwargs = {'num_labels': 3} if args.task == 'nli' else {}\n",
    "\n",
    "    # Here we select the right model fine-tuning head\n",
    "    model_classes = {'qa': AutoModelForQuestionAnswering,\n",
    "                        'nli': AutoModelForSequenceClassification}\n",
    "    model_class = model_classes[args.task]\n",
    "    # Initialize the model and tokenizer from the specified pretrained model/checkpoint\n",
    "    model = model_class.from_pretrained(args.model, **task_kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)\n",
    "\n",
    "    # Select the dataset preprocessing function (these functions are defined in helpers.py)\n",
    "    if args.task == 'qa':\n",
    "        prepare_train_dataset = lambda exs: prepare_train_dataset_qa(exs, tokenizer)\n",
    "        prepare_eval_dataset = lambda exs: prepare_validation_dataset_qa(exs, tokenizer)\n",
    "    elif args.task == 'nli':\n",
    "        prepare_train_dataset = prepare_eval_dataset = \\\n",
    "            lambda exs: prepare_dataset_nli(exs, tokenizer, args.max_length)\n",
    "        # prepare_eval_dataset = prepare_dataset_nli\n",
    "    else:\n",
    "        raise ValueError('Unrecognized task name: {}'.format(args.task))\n",
    "\n",
    "    print(\"Preprocessing data... (this takes a little bit, should only happen once per dataset)\")\n",
    "    if dataset_id == ('snli',):\n",
    "        # remove SNLI examples with no label\n",
    "        dataset = dataset.filter(lambda ex: ex['label'] != -1)\n",
    "\n",
    "    eval_dataset = None\n",
    "    train_dataset_featurized = None\n",
    "    eval_dataset_featurized = None\n",
    "    \n",
    "    if training_args.do_eval:\n",
    "        eval_dataset = dataset[eval_split]\n",
    "        if args.max_eval_samples:\n",
    "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "        eval_dataset_featurized = eval_dataset.map(\n",
    "            prepare_eval_dataset,\n",
    "            batched=True,\n",
    "            num_proc=2,\n",
    "            remove_columns=eval_dataset.column_names\n",
    "        )\n",
    "\n",
    "    # Select the training configuration\n",
    "    trainer_class = Trainer\n",
    "    eval_kwargs = {}\n",
    "    # If you want to use custom metrics, you should define your own \"compute_metrics\" function.\n",
    "    # For an example of a valid compute_metrics function, see compute_accuracy in helpers.py.\n",
    "    compute_metrics = None\n",
    "    if args.task == 'qa':\n",
    "        # For QA, we need to use a tweaked version of the Trainer (defined in helpers.py)\n",
    "        # to enable the question-answering specific evaluation metrics\n",
    "        trainer_class = QuestionAnsweringTrainer\n",
    "        eval_kwargs['eval_examples'] = eval_dataset\n",
    "        metric = datasets.load_metric('squad')\n",
    "        compute_metrics = lambda eval_preds: metric.compute(\n",
    "            predictions=eval_preds.predictions, references=eval_preds.label_ids)\n",
    "    elif args.task == 'nli':\n",
    "        compute_metrics = compute_accuracy\n",
    "\n",
    "\n",
    "    # This function wraps the compute_metrics function, storing the model's predictions\n",
    "    # so that they can be dumped along with the computed metrics\n",
    "    eval_predictions = None\n",
    "    def compute_metrics_and_store_predictions(eval_preds):\n",
    "        nonlocal eval_predictions\n",
    "        eval_predictions = eval_preds\n",
    "        return compute_metrics(eval_preds)\n",
    "\n",
    "    # Initialize the Trainer object with the specified arguments and the model and dataset we loaded above\n",
    "    trainer = trainer_class(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_featurized,\n",
    "        eval_dataset=eval_dataset_featurized,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_and_store_predictions\n",
    "    )\n",
    "    # Train and/or evaluate\n",
    "    if training_args.do_train:\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        # If you want to customize the way the loss is computed, you should subclass Trainer and override the \"compute_loss\"\n",
    "        # method (see https://huggingface.co/transformers/_modules/transformers/trainer.html#Trainer.compute_loss).\n",
    "        #\n",
    "        # You can also add training hooks using Trainer.add_callback:\n",
    "        #   See https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer.add_callback\n",
    "        #   and https://huggingface.co/transformers/main_classes/callback.html#transformers.TrainerCallback\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        results = trainer.evaluate(**eval_kwargs)\n",
    "\n",
    "        # To add custom metrics, you should replace the \"compute_metrics\" function (see comments above).\n",
    "        #\n",
    "        # If you want to change how predictions are computed, you should subclass Trainer and override the \"prediction_step\"\n",
    "        # method (see https://huggingface.co/transformers/_modules/transformers/trainer.html#Trainer.prediction_step).\n",
    "        # If you do this your custom prediction_step should probably start by calling super().prediction_step and modifying the\n",
    "        # values that it returns.\n",
    "\n",
    "        print('Evaluation results:')\n",
    "        print(results)\n",
    "\n",
    "        os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(training_args.output_dir, 'eval_metrics.json'), encoding='utf-8', mode='w') as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "        with open(os.path.join(training_args.output_dir, 'eval_predictions.jsonl'), encoding='utf-8', mode='w') as f:\n",
    "            if args.task == 'qa':\n",
    "                predictions_by_id = {pred['id']: pred['prediction_text'] for pred in eval_predictions.predictions}\n",
    "                for example in eval_dataset:\n",
    "                    example_with_prediction = dict(example)\n",
    "                    example_with_prediction['predicted_answer'] = predictions_by_id[example['id']]\n",
    "                    f.write(json.dumps(example_with_prediction))\n",
    "                    f.write('\\n')\n",
    "            else:\n",
    "                for i, example in enumerate(eval_dataset):\n",
    "                    example_with_prediction = dict(example)\n",
    "                    example_with_prediction['predicted_scores'] = eval_predictions.predictions[i].tolist()\n",
    "                    example_with_prediction['predicted_label'] = int(eval_predictions.predictions[i].argmax())\n",
    "                    f.write(json.dumps(example_with_prediction))\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d8a9957078c9c74f\n",
      "Reusing dataset json (/home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "custom_datadict = DatasetDict()\n",
    "custom_data = Dataset.from_json('custom_data/custom_eval.json')\n",
    "custom_datadict['validation'] = custom_data\n",
    "custom_datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0/cache-06c7867477a0d4d9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0/cache-8fbe281e6c09b00f.arrow\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0/cache-33de89b7c83ab5f2.arrow\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "{'eval_loss': 1.2181907892227173, 'eval_accuracy': 0.5, 'eval_runtime': 0.0589, 'eval_samples_per_second': 169.646, 'eval_steps_per_second': 33.929}\n"
     ]
    }
   ],
   "source": [
    "eval_model(custom_datadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_data(dataset):\n",
    "    with open('eval_args.pkl', 'rb') as filehandler:\n",
    "        training_args = pickle.load(filehandler)\n",
    "    with open('args.pkl', 'rb') as filehandler:\n",
    "        args = pickle.load(filehandler)\n",
    "\n",
    "    default_datasets = {'qa': ('squad',), 'nli': ('snli',)}\n",
    "    dataset_id = tuple(args.dataset.split(':')) if args.dataset is not None else \\\n",
    "        default_datasets[args.task]\n",
    "    # MNLI has two validation splits (one with matched domains and one with mismatched domains). Most datasets just have one \"validation\" split\n",
    "    eval_split = 'validation_matched' if dataset_id == ('glue', 'mnli') else 'validation'\n",
    "\n",
    "    # NLI models need to have the output label count specified (label 0 is \"entailed\", 1 is \"neutral\", and 2 is \"contradiction\")\n",
    "    task_kwargs = {'num_labels': 3} if args.task == 'nli' else {}\n",
    "\n",
    "    # Here we select the right model fine-tuning head\n",
    "    model_classes = {'qa': AutoModelForQuestionAnswering,\n",
    "                        'nli': AutoModelForSequenceClassification}\n",
    "    model_class = model_classes[args.task]\n",
    "    # Initialize the model and tokenizer from the specified pretrained model/checkpoint\n",
    "    model = model_class.from_pretrained(args.model, **task_kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)\n",
    "\n",
    "    # Select the dataset preprocessing function (these functions are defined in helpers.py)\n",
    "    if args.task == 'qa':\n",
    "        prepare_train_dataset = lambda exs: prepare_train_dataset_qa(exs, tokenizer)\n",
    "        prepare_eval_dataset = lambda exs: prepare_validation_dataset_qa(exs, tokenizer)\n",
    "    elif args.task == 'nli':\n",
    "        prepare_train_dataset = prepare_eval_dataset = \\\n",
    "            lambda exs: prepare_dataset_nli(exs, tokenizer, args.max_length)\n",
    "        # prepare_eval_dataset = prepare_dataset_nli\n",
    "    else:\n",
    "        raise ValueError('Unrecognized task name: {}'.format(args.task))\n",
    "\n",
    "    print(\"Preprocessing data... (this takes a little bit, should only happen once per dataset)\")\n",
    "    if dataset_id == ('snli',):\n",
    "        # remove SNLI examples with no label\n",
    "        dataset = dataset.filter(lambda ex: ex['label'] != -1)\n",
    "    \n",
    "    eval_dataset = dataset[eval_split]\n",
    "    if args.max_eval_samples:\n",
    "        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "    eval_dataset_featurized = eval_dataset.map(\n",
    "        prepare_eval_dataset,\n",
    "        batched=True,\n",
    "        num_proc=2,\n",
    "        remove_columns=eval_dataset.column_names\n",
    "    )\n",
    "\n",
    "    return eval_dataset_featurized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3fb92396f0bc704f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/richiew/.cache/huggingface/datasets/json/default-3fb92396f0bc704f/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d269c3a91a604cc0ba0e5666643a788a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d8a9957078c9c74f\n",
      "Reusing dataset json (/home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0)\n",
      "loading configuration file ./trained_model/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ./trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
      "loading file ./trained_model/vocab.txt\n",
      "loading file ./trained_model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./trained_model/special_tokens_map.json\n",
      "loading file ./trained_model/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/richiew/.cache/huggingface/datasets/json/default-3fb92396f0bc704f/0.0.0. Subsequent calls will reuse this data.\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3cb4f0f17e487dbd92b4782eb0abf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./trained_model/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ./trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
      "loading file ./trained_model/vocab.txt\n",
      "loading file ./trained_model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./trained_model/special_tokens_map.json\n",
      "loading file ./trained_model/tokenizer_config.json\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0/cache-06c7867477a0d4d9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0/cache-33de89b7c83ab5f2.arrow\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/json/default-d8a9957078c9c74f/0.0.0/cache-8fbe281e6c09b00f.arrow\n",
      "Reusing dataset snli (/home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b)\n",
      "loading configuration file ./trained_model/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-small-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.10.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./trained_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
      "\n",
      "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ./trained_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./trained_model/added_tokens.json. We won't load it.\n",
      "loading file ./trained_model/vocab.txt\n",
      "loading file ./trained_model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./trained_model/special_tokens_map.json\n",
      "loading file ./trained_model/tokenizer_config.json\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-eefb738451aa3c2b.arrow\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-ce177bf176742e21.arrow\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-47fbf995307229c8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-8ce2253536cb23ce.arrow\n",
      "Loading cached processed dataset at /home/richiew/.cache/huggingface/datasets/snli/plain_text/1.0.0/1f60b67533b65ae0275561ff7828aad5ee4282d0e6f844fd148d05d3c6ea251b/cache-8a38803079cdfd6c.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_datadict = DatasetDict()\n",
    "train_data = Dataset.from_json('custom_data/custom_train.json')\n",
    "train_datadict['validation'] = train_data\n",
    "\n",
    "test_datadict = DatasetDict()\n",
    "test_data = Dataset.from_json('custom_data/custom_eval.json')\n",
    "test_datadict['validation'] = test_data\n",
    "\n",
    "feat_train_dataset = featurize_data(train_datadict)\n",
    "feat_test_dataset = featurize_data(test_datadict)\n",
    "\n",
    "snli_dataset = featurize_data(datasets.load_dataset('snli'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:44:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.5554362  0.53062665 0.42164522 0.5554362  0.53062665 0.53062665\n",
      " 0.51278526 0.42164522 0.51278526 0.42164522]\n",
      "Acc:  0.7\n",
      "F1:  0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Setting the Parameters of the Model\n",
    "param = {'eta': 0.3,\n",
    "         'max_depth': 5,\n",
    "         'objective': 'binary:logistic',\n",
    "         'learning_rate': 0.1}\n",
    "\n",
    "xgb_train_labels = [1 if x == 0 else 0 for x in feat_train_dataset['label']]\n",
    "xgb_test_labels = [1 if x == 0 else 0 for x in feat_test_dataset['label']]\n",
    "\n",
    "xgb_train_inputs = xgb.DMatrix(feat_train_dataset['input_ids'], label=xgb_train_labels)\n",
    "xgb_test_inputs = xgb.DMatrix(feat_test_dataset['input_ids'], label=xgb_test_labels)\n",
    "\n",
    "# Training the Model\n",
    "xgb_model = xgb.train(param, xgb_train_inputs, 3)\n",
    "\n",
    "# Predicting using the Model\n",
    "y_pred = xgb_model.predict(xgb_test_inputs)\n",
    "print(y_pred)\n",
    "y_pred = np.where(np.array(y_pred) > 0.5, 1, 0)\n",
    "\n",
    "# Evaluation of Model\n",
    "print('Acc: ', accuracy_score(xgb_test_labels, y_pred))\n",
    "print('F1: ', f1_score(xgb_test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
